<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Scientific Computing in Practice 2015 Day 2: Debugging, Profiling, Optimizing</title>

		<meta name="description" content="Scientific Computing in Practice 2015 Day 2 Supplemental slides">
		<meta name="author" content="Janne Blomqvist">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section data-markdown>
					<script type="text/template">
					  # Scientific Computing in Practice 2015
					  ## Day 2
					  ### Low level dig down

					  Janne Blomqvist
					
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  Richard Darst has explained the basics of debugging, profiling, and optimizing using python. What follows here is an overview of what additional issues needs to be taken into account when working with lower level languages like C, C++, or Fortran, as well as an introduction to useful tools for these languages.

					  - On language choice
					  - Brief introduction to modern CPU and memory architectures
					  - Using the GNU debugger
					  - Using the perf profiler
					  - MPI debugging and profiling tools on Triton
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  # On language choice

					  ### Holy war time!

					  Typically, programming languages can be classified into high level and low level languages.

					  - High level languages such as Python emphasise programmer productivity, typically at the expense of performance.
					  - Low level languages such as C, C++, Fortran tend to be somewhat more aligned with how the hardware operates, and often lack safety features.
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ### Side note, Why not both high productivity and high performance?
					  
					  Indeed, why not?

					  See [Julia](http://julialang.org/) for a language that tries to do both, with an emphasis on technical computing. Available on triton via the module system. Typical performace roughly within a factor of 2 of C. How is it possible?

					  - Careful language design to create a high level dynamic language that can still be efficiently executed.
					  - Type inferencing to avoid (most) type dispatch at runtime.
					  - Just-in-Time (JIT) compiled to machine code rather than interpreted.
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  # Brief introduction to CPU and memory architecture

					  
					  - A brief history of recent trends in microprocessor design
					  - What are the relevant factors today affecting performance?
					  
					  
					</script>
				</section>

				<section>
				  <h3>von Neumann architecture: Abstract model of how a computer works:</h3>

				  <img src="Von_Neumann_Architecture.svg" style="background-color:white;">

				</section>

				<section data-markdown>
					<script type="text/template">
					  Modern processors are vastly complicated, with lots of features to increase performance

					  - Pipelined execution
					  - Superscalar
					  - Multiple levels of cache
					  - Branch prediction and speculative execution
					  - Prefetching
					  - Out-of-Order Execution
					  - SIMD instructions
					  - etc.
					</script>
				</section>

				<section>
				  <h4>Major functional blocks of Intel Core2 processor (~10 years old)</h4>
				  <img src="Intel_Core2_arch.svg" width="500" height="579" style="background-color:white;">
				</section>

				<section>
				  Something happened around 2005...
				  <img src="CPU.png" width="435" height="434" style="background-color:white;">
				  <p>
				    Switching power P ~ CfV<sup>2</sup> + P<sub>static</sub>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ### The result
					  - The "Power Wall" *combined with*
					  - Increasing ILP through microarchitectural tricks running out of steam

					  => Instead of new CPU's that could run existing (non-parallel) code faster we got *multiple cores*. Thus shifting the burden of improving performance from the CPU designers to the programmers (i.e. YOU!)
					</script>
				</section>

                                <section data-markdown>
					<script type="text/template">
					  ### Transistors vs. wires
					  - Old rule of thumb: Transistors expensive, wires cheap
					     - That is: Focus on arithmetic, don't worry about data movement.
					  - Energy cost of FP computing:
					     - DP FMA: 25 pJ/flop (read 3 operands, write 1)
					     - Registers: 5 pJ/flop
					     - Fetch DP from cache: 20-125 pJ
					     - Fetch DP from memory: 1 nJ
					</script>
				</section>

                                <section>
                                  <h3>Transistors vs. wires</h3>
				  
				  <p>
				    <ul>
				      <li>Current rule of thumb: Transistors cheap, wires expensive.
                                      <li>So was the old rule wrong all the time? No, <i>back when it was formulated</i> it was Ok. So what happened?
					<ul>
					  <li>Semiconductor scaling trends...
					</ul>
				      <li>This trend is continuing
				      <li>Processors are, and will become more so, bandwidth/latency starved rather than bottlenecked by arithmetic performance.
				    </ul>
                                </section>


				<section data-markdown>
					<script type="text/template">
					  ### Implications:
					  - Low level optimization these days is mostly about managing memory access
					     - Avoiding stalling on accessing main memory.
					     - Cache friendly algorithms
					     - Exposing Memory Level Parallelism (MLP)
					     - Register allocation? Don't worry, compilers are better at this than you..
					  - In HPC, vectorization can also be important. Increasingly so with newer processors.
					</script>
				</section>

				<section>
				  <h4>Latency Comparison Numbers (Jeff Dean, Google)</h4>
<code><pre>
L1 cache reference                            0.5 ns
Branch mispredict                             5   ns
L2 cache reference                            7   ns  14x L1 cache
Mutex lock/unlock                            25   ns
Main memory reference                       100   ns  20x L2 cache, 200x L1 cache
Compress 1K bytes with Snappy/LZ4         3,000   ns
Send 1K bytes over 1 Gbps network        10,000   ns  Triton IB network A LOT faster
Read 4K randomly from SSD*              150,000   ns
Read 1 MB sequentially from memory      250,000   ns
Round trip within same datacenter       500,000   ns  Triton IB network 5000ns
Read 1 MB sequentially from SSD*      1,000,000   ns
Disk seek                            10,000,000   ns
Read 1 MB sequentially from disk     20,000,000   ns
Send packet CA->Netherlands->CA     150,000,000   ns
</pre></code>

				</section>

				<section>
				  <h4>SCI Cluster node double precision FP throughput</h4>

				  <table>
				      <tr>
					<th>Generation<th>Clock (GHz)<th>CPU cores<th>Scalar thr (flops/core/cycle)<th>Vector thr (flops/core/cycle)
					<th>Total thr (GFlops/s)
				      </tr>
				      <tr>
					<td>Kvartsi2004<td>2.2<td>1<td>2<td>2 SSE2<td>8.8
				      </tr>
				      <tr>
					<td>Kvartsi2006<td>2.4<td>2<td>2<td>2<td>19.2
				      </tr>
				      <tr>
					<td>Triton2009<td>2.6<td>6<td>2<td>4 SSE2 hw<td>125
				      </tr>
				      <tr>
					<td>Triton2011<td>2.66<td>6<td>2<td>4<td>128
				      </tr>
				      <tr>
					<td>Triton2014<td>2.8<td>10<td>2<td>8 AVX<td>448
				      </tr>
				      <tr>
					<td>Triton2015?<td>2.6<td>12<td>2<td>16 AVX2<td>998
				      </tr>
				    </table>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ### Take home message (vectorization)
					  
					  During the past decade,
					  clock speed and scalar
					  throughput has remained
					  roughly
					  constant. Performance
					  improvement from
					  parallelization,
					  vectorization, and as of
					  Xeon Haswell generation,
					  also FMA (a*b+c operation).

					  
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  # Memory
					  
					  Pages, cachelines, prefetching, addressing, oh my!

					  Logically, memory is byte (1
					  byte == 8 bits)
					  addressable. Behind the
					  covers, a lot of stuff
					  happens.
					  
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ## Memory virtual addressing
					  
					  - Each process receives its own 64-bit virtual address space. 
					  - Virtual addresses are translated to actual physical addresses.
					  - A special data structure called a *page table* describes the mapping from virtual addresses to physical addresses. The OS sets up page tables when creating a process, and modifies them on request (brk()/sbrk()/mmap())
					  - The granularity of this virtual -> physical mapping is the *page size*. On x86, the standard page size is 4 kB, although so called hugepages (2MB, 1 GB) also possible and may be useful is some special circumstances.
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ### TLB
					  
					  - There is a special cache for virtual -> physical mappings, the *Translation Lookaside Buffer (TLB)*. 
					  - This cache is needed for *every* memory access, so needs to be very fast. However, it's quite small. => If you have an algorithm that accesses memory all over the place, performance may be bottlenecked by TLB misses.
					  
					  
					</script>
				</section>

				<section>
				  <h3>Accessing memory and virtual->physical mapping</h3>

				  <img src="Page_table_actions.svg" width="625" height="405" style="background-color:white;">

				</section>

				<section data-markdown>
					<script type="text/template">
					  #### Memory not mapped in page table
					  
					  - If the virtual address is not present in the page table mapping, the process will trap into the OS.
					  - If the OS determines that the virtual address was invalid, kills the program with SIGSEGV (segmentation fault).
					  - If the mapping is valid but this is the first time it's accessed, allocate physical memory and set up the page table mapping. *Note*: This happens when accessing the memory, not when executing malloc()/new/ALLOCATE/etc.!
					  - If the mapping is to a file (mmap() syscall), (read data from disk into page cache), setup page table mapping.
					  
					  
					</script>
				</section>

				<section>
				  <h3>Caching</h3>

				  <ul>
				    <li>Since memory is (relatively) slow, multiple levels of cache
				    <li>Typical contemporary configuration
				  </ul>
				  
				  <table>
				    <tr>
				      <th>Type<th>Size (kB)<th>Min latency (clock cycles)
				    </tr>
				    <tr>
				      <td>L1<td>32+32<td>4
				    </tr>
				    <tr>
				      <td>L2<td>256<td>11
				    </tr>
				    <tr>
				      <td>L3<td>2500 (per core, shared)<td>36
				    </tr>
				    <tr>
				      <td>main, local<td>-<td>200
				    </tr>
				    <tr>
				      <td>main, non-local<td>-<td>300(?)
				    </tr>
				    </table>

				</section>

				<section data-markdown>
					<script type="text/template">
					  ### Caching and prefetching

					  - Prefetchers: Hardware circuits that detect common memory access patterns, and fetch data from memory into cache (assuming the pattern continues).
					  
					  - Cacheline: granularity of
					  transfer from main memory to
					  caches. Typically 64 bytes
					  on current x86.
					     - Threads: Watch out for cacheline ping-ponging!
					  
					  - Caches, prefetching
					  etc. works well with code
					  that has nice predictable
					  access patterns and exhibit
					  *locality of reference*.

					  - Random access: Not so good
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ### Cache associativity
					  
					  - The cache can be thought of as consisting of a set of *slots*, each capable if holding a cacheline worth of data.
					  - Direct mapped cache: Each memory address can go into a single slot in the cache
					     - High possibility of *associativity conflict*
					  - Fully associative cache: Each memory address can go into any slot in the cache
					     - Complex, slow circuitry
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ### Set-associative cache
					  
					  - N-way set associative cache: Each memory address can go into N different slots in the cache. Essentially a middle ground between direct mapped and fully associative.
					     - Current CPU's typically have 4- or 8-way set associative caches
					  - Exercise: Consider a two-dimensional array (matrix) stored in column-major ordering (Fortran, MATLAB, Julia) where the number of rows is an even multiple of the cacheline size.
					     - Imagine accessing the matrix row-wise. What happens with the cache?
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  # References

					  - Hennessy, Patterson: Computer Architecture: A Quantitative Approach. 
					  - [Ulrich Drepper: What Every Programmer should know about memory](http://akkadia.org/drepper/cpumemory.pdf)
					  - [Sutter: The free lunch is over](http://www.gotw.ca/publications/concurrency-ddj.htm)
					  - [Sodani: Multicore trends in High Performance Computing](https://www.sics.se/sites/default/files/pub/sics.se/avinash_final_sweden_many_core_day_keynote_-_avinash_final_-_clean.pdf)
					  
					  - [Galal, Horowitz: Energy-Efficient Floating-Point Unit Design](http://dx.doi.org/10.1109/TC.2010.121)
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  # Debugging

					  - Debugging, the process of finding and correcting programming errors
					  - Richard has covered general debugging topics and python specific stuff
					     - This part is about debugging C/C++/Fortran code
					  - General debugging options for the compiler
					  - A few useful debugging tools
					  - GDB tutorial
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ## Compiler debug options

					  - Compilers have lots of switches to turn on warning and error messsages, use them!
					  - For GCC: <code>-Wall -Wextra -pedantic -Werror</code>
					  - GFortran: The above + <code>-fcheck=all -ffpe-trap=invalid,zero,overflow</code>. Note that <code>-fcheck=</code> makes the code *a lot* slower, so don't use it for production builds. Good for debugging, though. <code>-ffpe-trap=</code> might be problematic with LAPACK, otherwise useful.
					  - Intel compilers: See manual
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ## Illegal, undefined and implementation-defined behavior

					  - The C/C++/Fortran standards specify a lot of cases where if you do X, the result is illegal/undefined/implementation-defined.
					  - Illegal: Not allowed
					     - The compiler catches *some* of these, not all!
					     - Typical examples: Out-of-bounds access, type punning
					         - Type punning; GCC: <code>-fno-strict-aliasing</code>
					  

					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ### Undefined behavior

					  - Undefined behavior: Result is, well, undefined
					     - It might work (for some definition of *work*)
					     - Or it might not
					     - It might work differently depending on optimization level, compiler version, phase of the moon, whatever...
					     - Typical UB: reading uninitialized memory, integer overflow.
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ### Implementation-defined behavior

					  - Implementation-defined behavior: It should work, but result might differ for different compilers, OS's etc.
					     - Typical implementation-defined behavior: PRNG algorithm
					  - Reproducible research: Your code should not rely on illegal/undefined/implementation-defined behavior.
					     - A lot of research code is, sadly, *very* far from this goal...
					  
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ## Sanitizers

					  - Recent versions of GCC (and clang) support *sanitizers*
					  - <code>-fsanitize=xxx</code>
					  - address: Fast memory error detector, detect out-of-bounds access and use-after-free
					  - thread: Data race detector
					  - undefined: Catches many common cases of UB
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ## Valgrind

					  - Collection of debugging and profiling tools
					  - Most common use is the memory error detector
					  - Does not need any particular compile options (-g useful as always)
					  - Slows down execution *a lot* => Make sure you have a testcase that runs quickly!
					      $ valgrind ./a.out
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  # Introduction to GDB

					  - GDB, the GNU debugger, is the standard debugger on Linux for C, C++, Fortran and several other languages that compile to native code
					  - Continuously developed since 1986
					  - LOTS of features; Here we concentrate on a VERY SMALL subset of the most common operations
					  - Various graphical frontends also available. E.g. Eclipse, DDD.
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ## Why use a debugger

					  - printf() debugging: Insert print calls in your program, deduce where the bug is
					    - Simple
					    - ...but time-consuming
					  - A debugger lets you run the program, stop it where you want (breakpoints), inspect and modify the program state, etc.
					  
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ## Getting started

					  Always include -g in your compile options. This adds *debug symbols* to your executable.

					  - Even if you're not explicitly debugging your code. Makes debugging easier if you encounter an unexpected error later.
					  - Makes the executable bigger, but this is in practice never an issue in HPC
					  - With higher optimization levels a lot of transformations are done on the code
					     - Difficult to see how the code you're debugging corresponds to the source code. GCC 4.8+: -Og
					  
					</script>
				</section>

				<section>
				  <h2>Simple GDB example</h2>

				  <p>
				    foo.c:
				    <pre><code data-trim>
#include &lt;stdio.h>
int main()
{
	int *a = NULL;
	*a = 42;
	printf("%d\n", *a);
	return 0;
}
				    </code></pre>

				    <pre><code data-trim>
$ gcc -Og -g foo.c
$ ./a.out
Segmentation fault (core dumped)
$ gdb ./a.out
(gdb) r
Program received signal SIGSEGV, Segmentation fault.
main () at foo.c:5
5		*a = 42;
				    </code></pre>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ## Exercise

					  - Try the example from the previous slide yourself.
					  - Instead of GDB, try with valgrind and AddressSanitizer.
					  - For AddressSanitizer, you want to use [asan_symbolize.py](https://address-sanitizer.googlecode.com/svn-history/r908/trunk/scripts/asan_symbolize.py)
					      $ ./a.out 2>&1 | ./asan_symbolize.py

					  - When would you want to use GDB, valgrind, or AddressSanitizer? Which one is best?
					  - What happens without asan_symbolize.py? Why is it useful?
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ## Core dumps

					  - Remember the error message: Segmentation fault (core dumped)
					  - A *core* file is a snapshot of the process memory (at the time it crashed, typically).
					  - Often shell has core file limit set to 0 => No core files
					     - bash: ulimit -c unlimited
					  - Start GDB, load executable with associated core file:
					      $ gdb ./a.out core
					</script>
				</section>

				<section>
				  <h2>Core dumps</h2>
				  <p>
				    <ul>
				      <li>Core dump an existing process. Afterward, the process continues
					<pre><code>$ gcore PID</code></pre>
				      <li> Why is it called a <i>core dump</i>? What is core?
					<ul>
					  <li>Ferrite core memory, used in early computers...
					</ul>
				    </ul>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ## Breakpoints

					  - Execute a program until it hits a breakpoint, then pause it at that point and resume the debugger.
					  - Insert a breakpoint at line 5 of file foo.c, and on entry to function bar:

					      (gdb) break foo.c:5
					      (gdb) break bar
					  
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ## Single stepping, continuing

					  - Continue executing until hitting the next breakpoint, or end of program: <code>(gdb) continue</code>
					  - Execute next source line and pause: <code>(gdb) step</code>
					  - Like *step*, but proceed through subroutine calls: <code>(gdb) next</code>
					  - Short forms of the above: <code>c, s, n</code>
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ## Program state

					  - Value of a variable: <code>(gdb) print VAR</code>
					  - Modify value of a variable: <code>(gdb) set (VAR = VALUE)</code>
					  - Watchpoints (automatically print whenever value changes): <code>(gdb) watch VAR</code>
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ## Stack frames

					  - Print procedure call stack: <code>(gdb) backtrace</code>
					     - Short form: <code>bt</code>
					  - Jump up N stack frames: <code>(gdb) up N</code>
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ## Processes and threads

					  - Attach debugger to a running process: <code>$ gdb -p PID</code>
					     - Or inside gdb: <code>(gdb) attach PID</code>
					     - Note that this pauses the process!
					  - List current threads of process: <code>(gdb) info threads</code>
					  - Switch to another thread: <code>(gdb) thread N</code>
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ## Debugging multi-processes applications

					  - While GDB supports multi-threaded applications, there's no built-in support for multi-process applications
					  - MPI debugging with GDB:
					     - Launch MPI application
					     - For each MPI rank, start a terminal, start gdb attaching to the MPI process
					     - Debug!
					  - If you have a CSC account, you can use TotalView (MPI debugger) which is less cumbersome than the above..
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  # GDB References

					  - [GDB Manual](http://sourceware.org/gdb/current/onlinedocs/gdb/)
					  - [Beej's Quick Guide to GDB](http://beej.us/guide/bggdb/)
					</script>
				</section>
		

				<section data-markdown>
					<script type="text/template">
					  # Profiling

					  - Focus on perf
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  # Other tools

					  - valgrind
					  - [American fuzzy lop (afl)](http://lcamtuf.coredump.cx/afl/)
					</script>
				</section>
			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

		</script>

	</body>
</html>
