<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Scientific Computing in Practice 2015 Day 2: Debugging, Profiling, Optimizing</title>

		<meta name="description" content="Scientific Computing in Practice 2015 Day 2 Supplemental slides">
		<meta name="author" content="Janne Blomqvist">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section data-markdown>
					<script type="text/template">
					  # Scientific Computing in Practice 2015
					  ## Day 2
					  ### Low level dig down

					  Janne Blomqvist
					
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  Richard Darst has explained the basics of debugging, profiling, and optimizing using python. What follows here is an overview of what additional issues needs to be taken into account when working with lower level languages like C, C++, or Fortran, as well as an introduction to useful tools for these languages.

					  - On language choice
					  - Brief introduction to modern CPU and memory architectures
					  - Using the GNU debugger
					  - Using the perf profiler
					  - MPI debugging and profiling tools on Triton
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  # On language choice

					  ### Holy war time!

					  Typically, programming languages can be classified into high level and low level languages.

					  - High level languages such as Python emphasise programmer productivity, typically at the expense of performance.
					  - Low level languages such as C, C++, Fortran tend to be somewhat more aligned with how the hardware operates, and often lack safety features.
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ### Side note, Why not both high productivity and high performance?
					  
					  Indeed, why not?

					  See [Julia](http://julialang.org/) for a language that tries to do both, with an emphasis on technical computing. Available on triton via the module system. Typical performace roughly within a factor of 2 of C. How is it possible?

					  - Careful language design to create a high level dynamic language that can still be efficiently executed.
					  - Type inferencing to avoid (most) type dispatch at runtime.
					  - Just-in-Time (JIT) compiled to machine code rather than interpreted.
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  # Brief introduction to CPU and memory architecture

					  
					  - A brief history of recent trends in microprocessor design
					  - What are the relevant factors today affecting performance?
					  
					  
					</script>
				</section>

				<section>
				  <h3>von Neumann architecture: Abstract model of how a computer works:</h3>

				  <img src="Von_Neumann_Architecture.svg" style="background-color:white;">

				</section>

				<section data-markdown>
					<script type="text/template">
					  Modern processors are vastly complicated, with lots of features to increase performance

					  - Pipelined execution
					  - Superscalar
					  - Multiple levels of cache
					  - Branch prediction and speculative execution
					  - Prefetching
					  - Out-of-Order Execution
					  - SIMD instructions
					  - etc.
					</script>
				</section>

				<section>
				  <h4>Major functional blocks of Intel Core2 processor (~10 years old)</h4>
				  <img src="Intel_Core2_arch.svg" width="500" height="579" style="background-color:white;">
				</section>

				<section>
				  Something happened around 2005...
				  <img src="CPU.png" width="435" height="434" style="background-color:white;">
				  <p>
				    Switching power P ~ CfV<sup>2</sup> + P<sub>static</sub>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ### The result
					  - The "Power Wall" *combined with*
					  - Increasing ILP through microarchitectural tricks running out of steam

					  => Instead of new CPU's that could run existing (non-parallel) code faster we got *multiple cores*. Thus shifting the burden of improving performance from the CPU designers to the programmers (i.e. YOU!)
					</script>
				</section>

                                <section data-markdown>
					<script type="text/template">
					  ### Transistors vs. wires
					  - Old rule of thumb: Transistors expensive, wires cheap
					     - That is: Focus on arithmetic, don't worry about data movement.
					  - Energy cost of FP computing:
					     - DP FMA: 25 pJ/flop (read 3 operands, write 1)
					     - Registers: 5 pJ/flop
					     - Fetch DP from cache: 20-125 pJ
					     - Fetch DP from memory: 1 nJ
					</script>
				</section>

                                <section>
                                  <h3>Transistors vs. wires</h3>
				  
				  <p>
				    <ul>
				      <li>Current rule of thumb: Transistors cheap, wires expensive.
                                      <li>So was the old rule wrong all the time? No, <i>back when it was formulated</i> it was Ok. So what happened?
					<ul>
					  <li>Semiconductor scaling trends...
					</ul>
				      <li>This trend is continuing
				      <li>Processors are, and will become more so, bandwidth/latency starved rather than bottlenecked by arithmetic performance.
				    </ul>
                                </section>


				<section data-markdown>
					<script type="text/template">
					  ### Implications:
					  - Low level optimization these days is mostly about managing memory access
					     - Avoiding stalling on accessing main memory.
					     - Cache friendly algorithms
					     - Register allocation? Don't worry, compilers are better at this than you..
					  - In HPC, vectorization can also be important. Increasingly so with newer processors.
					</script>
				</section>

				<section>
				  <h4>Latency Comparison Numbers (Jeff Dean, Google)</h4>
<code><pre>
L1 cache reference                            0.5 ns
Branch mispredict                             5   ns
L2 cache reference                            7   ns  14x L1 cache
Mutex lock/unlock                            25   ns
Main memory reference                       100   ns  20x L2 cache, 200x L1 cache
Compress 1K bytes with Snappy/LZ4         3,000   ns
Send 1K bytes over 1 Gbps network        10,000   ns  Triton IB network A LOT faster
Read 4K randomly from SSD*              150,000   ns
Read 1 MB sequentially from memory      250,000   ns
Round trip within same datacenter       500,000   ns  Triton IB network 5000ns
Read 1 MB sequentially from SSD*      1,000,000   ns
Disk seek                            10,000,000   ns
Read 1 MB sequentially from disk     20,000,000   ns
Send packet CA->Netherlands->CA     150,000,000   ns
</pre></code>

				</section>

				<section>
				  <h4>SCI Cluster node double precision FP throughput</h4>

				  <table>
				      <tr>
					<th>Generation<th>Clock (GHz)<th>CPU cores<th>Scalar thr (flops/core/cycle)<th>Vector thr (flops/core/cycle)
					<th>Total thr (GFlops/s)
				      </tr>
				      <tr>
					<td>Kvartsi2004<td>2.2<td>1<td>2<td>2 SSE2<td>8.8
				      </tr>
				      <tr>
					<td>Kvartsi2006<td>2.4<td>2<td>2<td>2<td>19.2
				      </tr>
				      <tr>
					<td>Triton2009<td>2.6<td>6<td>2<td>4 SSE2 hw<td>125
				      </tr>
				      <tr>
					<td>Triton2011<td>2.66<td>6<td>2<td>4<td>128
				      </tr>
				      <tr>
					<td>Triton2014<td>2.8<td>10<td>2<td>8 AVX<td>448
				      </tr>
				      <tr>
					<td>Triton2015?<td>2.6<td>12<td>2<td>16 AVX2<td>998
				      </tr>
				    </table>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ### Take home message (vectorization)
					  
					  During the past decade,
					  clock speed and scalar
					  throughput has remained
					  roughly
					  constant. Performance
					  improvement from
					  parallelization,
					  vectorization, and as of
					  Xeon Haswell generation,
					  also FMA (a*b+c operation).

					  
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  # Memory
					  
					  Pages, cachelines, prefetching, addressing, oh my!

					  Logically, memory is byte (1
					  byte == 8 bits)
					  addressable. Behind the
					  covers, a lot of stuff
					  happens.
					  
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ## Memory virtual addressing
					  
					  - Each process receives its own 64-bit virtual address space. 
					  - Virtual addresses are translated to actual physical addresses.
					  - A special data structure called a *page table* describes the mapping from virtual addresses to physical addresses. The OS sets up page tables when creating a process, and modifies them on request (brk()/sbrk()/mmap())
					  - The granularity of this virtual -> physical mapping is the *page size*. On x86, the standard page size is 4 kB, although so called hugepages (2MB, 1 GB) also possible and may be useful is some special circumstances.
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ### TLB
					  
					  - There is a special cache for virtual -> physical mappings, the *Translation Lookaside Buffer (TLB)*. 
					  - This cache is needed for *every* memory access, so needs to be very fast. However, it's quite small. => If you have an algorithm that accesses memory all over the place, performance may be bottlenecked by TLB misses.
					  
					  
					</script>
				</section>

				<section>
				  <h3>Accessing memory and virtual->physical mapping</h3>

				  <img src="Page_table_actions.svg" width="625" height="405" style="background-color:white;">

				</section>

				<section data-markdown>
					<script type="text/template">
					  #### Memory not mapped in page table
					  
					  - If the virtual address is not present in the page table mapping, the process will trap into the OS.
					  - If the OS determines that the virtual address was invalid, kills the program with SIGSEGV (segmentation fault).
					  - If the mapping is valid but this is the first time it's accessed, allocate physical memory and set up the page table mapping. *Note*: This happens when accessing the memory, not when executing malloc()/new/ALLOCATE/etc.!
					  - If the mapping is to a file (mmap() syscall), (read data from disk into page cache), setup page table mapping.
					  
					  
					</script>
				</section>

				<section>
				  <h3>Caching</h3>

				  <ul>
				    <li>Since memory is (relatively) slow, multiple levels of cache
				    <li>Typical contemporary configuration
				  </ul>
				  
				  <table>
				    <tr>
				      <th>Type<th>Size (kB)<th>Min latency (clock cycles)
				    </tr>
				    <tr>
				      <td>L1<td>32+32<td>4
				    </tr>
				    <tr>
				      <td>L2<td>256<td>11
				    </tr>
				    <tr>
				      <td>L3<td>2500 (per core, shared)<td>36
				    </tr>
				    <tr>
				      <td>main, local<td>-<td>200
				    </tr>
				    <tr>
				      <td>main, non-local<td>-<td>300(?)
				    </tr>
				    </table>

				</section>

				<section data-markdown>
					<script type="text/template">
					  ### Caching and prefetching

					  - Prefetchers: Hardware circuits that detect common memory access patterns, and fetch data from memory into cache (assuming the pattern continues).
					  
					  - Cacheline: granularity of
					  transfer from main memory to
					  caches. Typically 64 bytes
					  on current x86.
					  
					  - Caches, prefetching
					  etc. works well with code
					  that has nice predictable
					  access patterns and exhibit
					  *locality of reference*.

					  - Random access: Not so good
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  # References

					  - Hennessy, Patterson: Computer Architecture: A Quantitative Approach. 
					  - [Ulrich Drepper: What Every Programmer should know about memory](http://akkadia.org/drepper/cpumemory.pdf)
					  - [Sutter: The free lunch is over](http://www.gotw.ca/publications/concurrency-ddj.htm)
					  - [Sodani: Multicore trends in High Performance Computing](https://www.sics.se/sites/default/files/pub/sics.se/avinash_final_sweden_many_core_day_keynote_-_avinash_final_-_clean.pdf)
					  
					  - [Galal, Horowitz: Energy-Efficient Floating-Point Unit Design](http://dx.doi.org/10.1109/TC.2010.121)
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  # Introduction to GDB

					  - GDB, the GNU debugger, is the standard debugger on Linux for C, C++, Fortran and several other languages that compile to native code
					  - Continuously developed since 1986
					  - LOTS of features; Here we concentrate on a VERY SMALL subset of the most common operations
					  - Various graphical frontends also available. E.g. Eclipse, DDD.
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ## Why use a debugger

					  - printf() debugging: Insert print calls in your program, deduce where the bug is
					    - Simple
					    - ...but time-consuming
					  - A debugger lets you run the program, stop it where you want (breakpoints), inspect and modify the program state, etc.
					  
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  ## Getting started

					  Always include -g in your compile options. This adds *debug symbols* to your executable.

					  - Even if you're not explicitly debugging your code. Makes debugging easier if you encounter an unexpected error later.
					  - Makes the executable bigger, but this is in practice never an issue in HPC
					  - With higher optimization levels a lot of transformations are done on the code
					      - Difficult to see how the code you're debugging corresponds to the source code.
					  
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  # GDB References

					  - [GDB Manual](http://sourceware.org/gdb/current/onlinedocs/gdb/)
					  - [Beej's Quick Guide to GDB](http://beej.us/guide/bggdb/)
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  # Profiling

					  - Focus on perf
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
					  # Other tools

					  - valgrind
					  - {Address, thread, undefined behavior} sanitizer
					  - [American fuzzy lop (afl)](http://lcamtuf.coredump.cx/afl/)
					</script>
				</section>
			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

		</script>

	</body>
</html>
